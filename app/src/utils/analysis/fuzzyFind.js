import levenshtein from '~/utils/analysis/levenshtein'

// since the text generated by the PDF renderer is different from text converted
// by the node library, we can try to normalize punctuation
function normalize(word) {
  return word.toLowerCase().replace(/[^a-z0-9_\- ]/g,'').trim()
}

export default function fuzzyFind(needle, haystack) {
  const needleStr      = normalize(needle)
  const needleArr      = needleStr.split(' ')
  const needleLength   = needleArr.length
  const haystackArr    = haystack.split(' ')
  const haystackLength = Math.max(1, haystack.length)
  let buff = 0
  for (let i = 0; i < haystackArr.length; i++) {
    const hayStr = haystackArr.slice(i, i + needleLength).join(' ')
    const cleanHayStr = normalize(hayStr)

    /*
    if (levenshtein(needleStr, cleanHayStr) < 4) {
      console.log(JSON.stringify({
        i: i,
        hayStr: hayStr,
        cleanHayStr: cleanHayStr,
        needleStr: needleStr,
        //buff: buff,
        rawDist: levenshtein(needleStr, cleanHayStr),
        cleanDist: levenshtein(needleStr, cleanHayStr)
      }, null, 2))
    }
    */

    // TODO replace by a levenshtein maybe?
    if (needleStr === cleanHayStr) {
      return {
        begin: buff,
        end  : buff + hayStr.length,
        left : buff / haystackLength,
        width: hayStr.length / haystackLength,
        right: (buff + hayStr.length) / haystackLength
      }
    }
    buff += haystackArr[i].length + 1
  }
  //const index = tem.str.indexOf(next.properties.ngram)
  //const distance = levenshtein(item.str, next.properties.ngram)
}
